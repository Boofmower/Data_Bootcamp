\documentclass[11pt]{article}

\oddsidemargin=0.25truein \evensidemargin=0.25truein
\topmargin=-0.5truein \textwidth=6.0truein \textheight=8.75truein

%\usepackage{graphicx}
\usepackage{comment}
%\usepackage{amssymb}
%\usepackage{amsfonts}
%\usepackage{booktabs}
\usepackage[small, compact]{titlesec}

\usepackage{hyperref}
\hypersetup{
    letterpaper=true,
    colorlinks=true,        % kills boxes
    allcolors=blue,
%    pdftitle={Demography and Capital Flows},
%    pdfsubject={Sources of Entropy},
    pdfauthor={Dave Backus @ NYU},
    pdfstartview={FitH},
    pdfpagemode={UseNone},
    pdfnewwindow=true,      % links in new window
%    pdfpagelabels=true,
%    linkcolor=blue,         % color of internal links
%    citecolor=blue,         % color of links to bibliography
%    filecolor=blue,         % color of file links
%    urlcolor=blue           % color of external links
% see:  http://www.tug.org/applications/hyperref/manual.html
}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\newcommand{\var}{\mbox{\it Var\/}}

% document starts here
\begin{document}
\parskip=\bigskipamount
\parindent=0.0in
\thispagestyle{empty}
\begin{flushright} Dave Backus @ NYU \end{flushright}

\bigskip
\centerline{\Large \bf Notes on Factor Models%
\footnote{Working notes, no guarantee of accuracy or sense.}}
\centerline{(Started: October 31, 2011; Revised: \today)}

\bigskip
This pulls together some thoughts on topics related
to factor models.
The idea throughout is to reduce the dimension
of the data and consider the various issues that come up when dealing with 
latent factors.  


\section{Transformations and rotations of multivariate normals}

There are some common issues referred to as identification, 
but they're typically closer to normalizations.  
Here's the simplest example I could think of:  
an $n$-dimensional random vector $y \sim \mathcal{N} (0,V)$ whose 
covariance matrix $V = E(y y^\top)$ has rank $ n$.  
The question is whether we can connect $y$ to a vector $w$ of independent standard normals.

In fact, we can do this in more than one way. 
The Cholesky decomposition gives us 
$ V =  L L^\top $ with L lower triangular.
Then $y = L w$ has covariance $V$ and $ w = L^{-1} y$. 
We can also use any 
\href{http://en.wikipedia.org/wiki/Orthogonal_matrix}{orthogonal transformation} of $w$.
For example, let $y = L Q w$, 
with $Q$ an orthogonal matrix ($Q^\top Q = Q Q^\top = I$) 
Then $ E (y y^\top) = (LQ)(LQ)^\top = L L^\top = V$. 
The transformation tends to kill the lower triangularity, 
but it shows ut that we have lots of options here.  
 
Another example is the spectral decomposition:  
$ V = Q D Q^\top$ with $Q$ (again) orthogonal and $D$ is diagonal.  
Here $Q$'s columns are the eigenvectors of $V$
and the elements of $D$ are the eigenvalues.  
If we define $T = Q D^{1/2}$, then $y = T w$ has covariance $V$.
We can find the $w$'s indirectly from $ w = T^{-1} y$. 


Both decompositions reduce the number of parameters.
If we observe $y$, then we can estimate $V$.
Since $V$ is symmetric, that gives us $n (n+1)/2$ pieces of information.
So we can't estimate more parameters than that.
In the Cholesky decomposition, we kill off the extra parameters 
by zeroing out the elements above the diagonal.
In the spectral decomposition, 
we accomplish the same thing with othogonality conditions among the columns.  




\section{Factor analysis}

A lot of this is adapted from Anderson and Rubin,
``\href{http://dido.wss.yale.edu/P/cp/p01a/p0103.pdf}
{Statistical inference in factor analysis},'' 1956,
which is a thing of beauty.

Consider an $n$-dimensional vector of data $y$ (``indicators'')
with mean zero and variance
$  E  ( y y^\top ) = V_y  $.
The idea of factor analysis is to approximate $y$
with a vector $x$ (``factors'') of dimension $k < n$:
\begin{eqnarray}
    y    &=& B x + u .
    \label{eq:factor-structure}
\end{eqnarray}
Here $ E(x x^\top) = V_x$, 
$E (u u^\top) = D $ is diagonal,
and $(x,u)$ are independent.
That implies a restricted variance matrix
\begin{eqnarray}
    V_y  &=& B E ( x x^\top ) B^\top + D
            \;\;=\;\; B V_x B^\top + D .
    \label{eq:variance-factor-structure}
\end{eqnarray}
Special case:  $D = 0$. 

The question again is whether we can identify the $x$'s
and the parameters that connect them to the $y$'s.  

This places some structure on covariance matrix,
which you can see by counting parameters.
$S$ has (since it's symmetric) $ n(n+1)/2$ parameters.
The rhs has $nq + n$,
which is smaller if
$ q < (n-1)/2 $.
This overstates the rhs, because for large $q$,
$B B^\top$ doesn't need $nq$ parameters.
[What's the logic here?]



\section{Principal components}

Take the same starting point:  a vector $y$ with
mean zero and variance $S$.
The spectral decomposition gives us
\begin{eqnarray*}
    S &=& Q D Q^\top
        \;\;=\;\; d_1 v_1 v_1^\top + d_2 v_2 v_2^\top  + \cdots
            + d_n v_n v_n^\top ,
\end{eqnarray*}
where $v_j$ is the $j$th column of $Q$.
If we rank the eigenvalues $d_j$ from largest to smallest,
we can use (say) the first $k$ to approximate $S$:
\begin{eqnarray*}
    S  &\approx&  d_1 v_1 v_1^\top + d_2 v_2 v_2^\top  + \cdots
            + d_k v_k v_k^\top .
\end{eqnarray*}
We can think of the $v$'s as factors.
Where factor analysis gives us $S = \mbox{factors} + \mbox{diagonal}$,
this gives us
$S = \mbox{factors} + \mbox{small}$.

[need to link up notation with what follows]

We can also interpret principal components as approximations to the
data matrix $Y$,
an $m$ by $n$ matrix whose rows are observations of $y^\top$.
The singular value decomposition gives us
\begin{eqnarray*}
    Y &=& Q_1 \Sigma Q_2^\top .
%        \;\;=\;\; u_1 \sigma_1 v_1^\top + u_2 \sigma_2 v_2^\top + \cdots
%        + u_n \sigma_n v_n^\top .
\end{eqnarray*}
If we estimate $S$ by $Y^\top Y$
(ignore for now division by number of observations),
we have $ \Sigma = Y^\top Y = Q_2 \Sigma^\top \Sigma Q_2^\top$.
That gives us the earlier decomposition with $D = \Sigma^\top \Sigma$
and $Q = Q_2$.

Now consider  the expansion
\begin{eqnarray*}
    Y &=&  u_1 \sigma_1 v_1^\top + u_2 \sigma_2 v_2^\top + \cdots
        + u_n \sigma_n v_n^\top .
\end{eqnarray*}
Let's truncate after $k$ terms.
That involves dropping columns of $\Sigma$ and the associated rows of
$Q_2^\top$.
So the truncated versions:
$\widehat{\Sigma}$ is $m$ by $k$ and
$\widehat{Q}_2^\top $ is $k$ by $n$.
The rows of $\widehat{Q}_2^\top $ are the factors.
The loadings are the diagonal matrix $\widehat{\Sigma}$,
so we've saved some parameters.
I think this gives us a basis for a clear
count of identifiable parameter values, but I haven't worked it out yet.
We could do the same with the version based on $S$.


%\pagebreak
\appendix
\section{Appendix on linear algebra results}

{\bf Orthogonal matrix.}  We say $Q$ is orthogonal (or sometimes, orthonormal)
if it's square and $ Q^\top Q = Q Q^\top = I$.

{\bf QR decomposition.}
If $A$ is $m$ by $n$ with $m \ge m$, it can be factored into $A = QR$
with $Q$ $m$ by $n$ and $Q^\top Q = I$ and $R$ upper triangular.
The space spanned by the columns of $A$ is the same as that spanned by the columns of $Q$.

A more complete version:
\begin{eqnarray*}
    A &=& QR \;\;=\;\;
        \left[ Q_1 \; Q_2 \right]
        \left[
        \begin{array}{c}
        R_1 \\ 0
        \end{array}
        \right]
\end{eqnarray*}
with $Q$ $m$ by $m$ orthogonal and $R$ upper triangular.
What we had before was the ``thin'' factorization $A = QR = Q_1 R_1 $.

{\bf Spectral decomposition.}
Any real symmetric matrix $A$ can be factored into $ Q \Lambda Q^\top$.
Here $\Lambda$ is a diagonal matrix containing the (real) eigenvalues of $A$
and $Q$ is orthogonal with columns equal to the eigenvectors.
See Strang, {\it Linear Algebra and Its Applications 3e\/}, Section 5.5.
Using ``column times row multiplication'' (Strang problem 1.4.21),
we have
\begin{eqnarray*}
    A &=& Q \Lambda Q^\top
        \;\;=\;\; \lambda_1 v_1 v_1^\top + \lambda_2 v_2 v_2^\top  + \cdots
            + \lambda_n v_n v_n^\top ,
\end{eqnarray*}
where $v_j$ is the $j$th column of $Q$ and $n$ is the dimension of $A$.
If $A$ is positive definite, the eigenvalues are positive.

{\bf Singular value decomposition.}
Taken from Strang, {\it Linear Algebra and Its Applications 3e\/}, Appendix A.
Let $A$ be $m$ by $n$.
Then it can be factored into $A = Q_1 \Sigma Q_2^\top $
with $Q_1, Q_2$ orthonormal and $\Sigma$ diagonal (not necessarily square) whose elements
are called ``singular values.''
Matching up dimensions, $Q_1$ is $n$ by $n$,
$Q_2$ is $m$ by $m$,
and $\Sigma$ is $n$ by $m$.

Examples and remarks:
\begin{itemize}
\item Strang's Example 1:
\begin{eqnarray*}
    A &=& \left[ \begin{array}{lr} 2 & 0 \\ 0 & -3  \\ 0 & 0 \end{array} \right]
    \;\;=\;\;
        \left[ \begin{array}{rrr} 1 & 0 & 0 \\ 0 & -1 & 0 \\ 0 & 0 & 1 \end{array} \right]
        \left[ \begin{array}{rr} 2 & 0 \\ 0 & 3 \\ 0 & 0 \end{array} \right]
        \left[ \begin{array}{rr} 1 & 0 \\ 0 & 1 \end{array} \right] .
\end{eqnarray*}

\item Strang's Example 2:
\begin{eqnarray*}
    A &=& \left[ \begin{array}{r} -1 \\ 2  \\ 2 \end{array} \right]
    \;\;=\;\;
        \left[ \begin{array}{rrr} -1/3 & 2/3 & 2/3 \\ 2/3 & -1/3 & 2/3 \\ 2/3 & 2/3 & -1/3 \end{array} \right]
        \left[ \begin{array}{r} 3 \\ 0  \\ 0 \end{array} \right]
        \left[ 1 \right]  .
\end{eqnarray*}

\item Symmetric case.  We get $ A = Q \Lambda Q^\top$, the spectral decomposition.

\item Matrix and eigenvalue connections:
\begin{eqnarray*}
    A A^\top \;\;=\;\; Q_1 \Sigma \Sigma^\top Q_1^\top,
    \;\;\;\;\;
    A^\top A \;\;=\;\; Q_2 \Sigma^\top \Sigma Q_2^\top .
\end{eqnarray*}
Eigenvalues are squares of those of $A$.
More precisely:  if $\lambda$ is an eigenvalue of $A^\top A$ with eigenvalue $v$,
then either (i)~$\lambda$ is an eigenvalue of $A A ^\top$ with eigenvalue $A v$ or
(ii)~$\lambda=0$ and $Av = 0$.

\item Dimension reduction 1.
If an element of $\Sigma$ is zero,
we can just skip that dimension and reduce the size
of the relevant matrices.
An example is this result from Brillinger (exercise 3.10.36, p 87).
Let $A$ be $j$ by $k$ with rank $l$.
Then we can write $A=BC$ for $B$ $j$ by $l$ and $C$ $l$ by $k$.
Think of $C$ as the product of $\Sigma$ and $Q_2^\top$, appropriately simplified.

\item Dimension reduction 2.  Suppose you want to approximate a high-dimensional $A$.
If the colums of $Q_1$ are $u_j$ and those of $Q_2$ are $v_j$ (ie, the rows of $Q_2^\top$), then ``column times row multiplication'' gives us
\begin{eqnarray*}
    A &=& Q_1 \Sigma Q_2^\top
        \;\;=\;\; u_1 \sigma_1 v_1^\top + u_2 \sigma_2 v_2^\top + \cdots + u_m \sigma_m v_m^\top .
\end{eqnarray*}
We can rank these by $\sigma_j$ and take only those with large values,
thus decreasing the amount of information we need to keep track of.


\item Space spanned by (columns of) $A$ same as that spanned by $Q_1$.
\item Postmultiply by $Q_2$, get $ A Q_2 = Q_1 \Sigma$, scaled columns of $Q_1$.

\end{itemize}


\end{document}

Cleve Moler on SVD (video):  \url{http://youtu.be/R9UoFyqJca8} \\
More Cleve: \url{http://www.mathworks.com/tagteam/35906_91425v00_clevescorner.pdf}
http://math.stackexchange.com/questions/3869/what-is-the-intuitive-relationship-between-svd-and-pca
http://www.mathworks.com/company/newsletters/articles/professor-svd.html

http://www.scholarpedia.org/article/Eigenfaces


